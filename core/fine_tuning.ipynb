{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37307667",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Model Fine-Tuning with Custom Webcam Dataset</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050660f",
   "metadata": {},
   "source": [
    "\n",
    "This notebook fine-tunes the pre-trained MyCNN model using additional custom data collected from webcam recordings (check ../scripts/). We explore different fine-tuning strategies to improve emotion detection performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f72ae7",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Dataset Structure</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec79c319",
   "metadata": {},
   "source": [
    "\n",
    "Custom dataset containing 48√ó48 grayscale images organized by emotion class. All images are preprocessed to match the model's input requirements:\n",
    "\n",
    "```bash\n",
    "dataset/\n",
    "    0_happy/\n",
    "        img_run1_0000.jpg\n",
    "        img_run1_0001.jpg\n",
    "        ...\n",
    "    1_sad/\n",
    "        img_run1_0000.jpg\n",
    "        ...\n",
    "    2_neutral/\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df333e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from model import MyCNN\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Grayscale(),\n",
    "    T.Resize((48, 48)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e8d006",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Load Dataset</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a87bb",
   "metadata": {},
   "source": [
    "\n",
    "Load the custom dataset from disk with appropriate image transformations (grayscale conversion, resizing, normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3485b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(\"../dataset\", transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e5543",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Load Pre-trained Model</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d8180b",
   "metadata": {},
   "source": [
    "\n",
    "Initialize the model and load the original weights from `best_model.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38073f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyCNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.05, inplace=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyCNN().to(device)\n",
    "state = torch.load(\"models/best_model.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(state, strict=False)  # Use strict=False to handle any architecture changes\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3778bd61",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Strategy: Conservative Fine-Tuning</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ad53c",
   "metadata": {},
   "source": [
    "**Trade-off Analysis:**\n",
    "- **Full model fine-tuning**: Risk of *catastrophic forgetting* (losing original learned features)\n",
    "- **Classifier-only fine-tuning**: Safer but limited to existing feature representations\n",
    "\n",
    "**Approach:** Decided to start conservatively by freezing all convolutional layers and training only the classifier head. If results are insufficient, progressively unfreeze conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfa24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all convolutional layers to preserve learned features\n",
    "for param in model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.conv3.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Only train the classifier (fully connected layers)\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a1a9b",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Training Loop (Round 1: Classifier Only)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197ba8b3",
   "metadata": {},
   "source": [
    "\n",
    "Train the classifier head with frozen convolutional layers. Monitor loss and accuracy over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d60f9349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 1.0676 | Train Acc: 65.2720 \n",
      "Epoch [2/20] Train Loss: 1.0476 | Train Acc: 64.4351 \n",
      "Epoch [3/20] Train Loss: 0.9315 | Train Acc: 69.0377 \n",
      "Epoch [4/20] Train Loss: 0.8825 | Train Acc: 69.8745 \n",
      "Epoch [5/20] Train Loss: 0.8594 | Train Acc: 67.7824 \n",
      "Epoch [6/20] Train Loss: 0.9060 | Train Acc: 66.9456 \n",
      "Epoch [7/20] Train Loss: 0.8217 | Train Acc: 68.2008 \n",
      "Epoch [8/20] Train Loss: 0.7935 | Train Acc: 69.8745 \n",
      "Epoch [9/20] Train Loss: 0.7963 | Train Acc: 66.5272 \n",
      "Epoch [10/20] Train Loss: 0.7699 | Train Acc: 71.5481 \n",
      "Epoch [11/20] Train Loss: 0.7494 | Train Acc: 71.1297 \n",
      "Epoch [12/20] Train Loss: 0.7233 | Train Acc: 68.6192 \n",
      "Epoch [13/20] Train Loss: 0.7296 | Train Acc: 70.7113 \n",
      "Epoch [14/20] Train Loss: 0.7147 | Train Acc: 71.1297 \n",
      "Epoch [15/20] Train Loss: 0.7297 | Train Acc: 70.2929 \n",
      "Epoch [16/20] Train Loss: 0.7309 | Train Acc: 70.7113 \n",
      "Epoch [17/20] Train Loss: 0.7053 | Train Acc: 71.1297 \n",
      "Epoch [18/20] Train Loss: 0.7077 | Train Acc: 70.2929 \n",
      "Epoch [19/20] Train Loss: 0.6823 | Train Acc: 71.5481 \n",
      "Epoch [20/20] Train Loss: 0.6758 | Train Acc: 71.5481 \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.fc.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total * 100\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ca53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"models/fine_tuned_classifier.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f8542",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Results & Analysis (Round 1)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54714d24",
   "metadata": {},
   "source": [
    "\n",
    "Evaluate performance metrics and identify weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bfbe7",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Happy detection improved significantly\n",
    "- Sad detection degraded further‚Äîthe model struggles with this class (later will know why hehe)\n",
    "\n",
    "**Action Items:**\n",
    "1. Unfreeze `conv3` (the last convolutional block) to allow deeper adaptation\n",
    "2. Increase training data for sad emotion (oversample)\n",
    "3. Adjust learning rate and epochs for better convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7818ed6",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Training Loop (Round 2: Partial Unfreezing)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972540f9",
   "metadata": {},
   "source": [
    "\n",
    "Unfreeze the last convolutional layer (`conv3`) to allow more adaptive feature learning while preserving early-stage features from `conv1` and `conv2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad3ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25] Train Loss: 0.9587 | Train Acc: 48.0573 \n",
      "Epoch [2/25] Train Loss: 0.8718 | Train Acc: 49.2843 \n",
      "Epoch [3/25] Train Loss: 0.8588 | Train Acc: 50.3067 \n",
      "Epoch [4/25] Train Loss: 0.8452 | Train Acc: 50.1022 \n",
      "Epoch [5/25] Train Loss: 0.7754 | Train Acc: 51.9427 \n",
      "Epoch [6/25] Train Loss: 0.7422 | Train Acc: 52.5562 \n",
      "Epoch [7/25] Train Loss: 0.7275 | Train Acc: 56.0327 \n",
      "Epoch [8/25] Train Loss: 0.6817 | Train Acc: 57.8732 \n",
      "Epoch [9/25] Train Loss: 0.7013 | Train Acc: 56.4417 \n",
      "Epoch [10/25] Train Loss: 0.6613 | Train Acc: 58.4867 \n",
      "Epoch [11/25] Train Loss: 0.6346 | Train Acc: 64.0082 \n",
      "Epoch [12/25] Train Loss: 0.6105 | Train Acc: 62.5767 \n",
      "Epoch [13/25] Train Loss: 0.5885 | Train Acc: 68.9162 \n",
      "Epoch [14/25] Train Loss: 0.5953 | Train Acc: 65.0307 \n",
      "Epoch [15/25] Train Loss: 0.5734 | Train Acc: 69.1207 \n",
      "Epoch [16/25] Train Loss: 0.5409 | Train Acc: 70.5521 \n",
      "Epoch [17/25] Train Loss: 0.5305 | Train Acc: 72.3926 \n",
      "Epoch [18/25] Train Loss: 0.4966 | Train Acc: 73.2106 \n",
      "Epoch [19/25] Train Loss: 0.4995 | Train Acc: 76.2781 \n",
      "Epoch [20/25] Train Loss: 0.4862 | Train Acc: 78.3231 \n",
      "Epoch [21/25] Train Loss: 0.4543 | Train Acc: 80.7771 \n",
      "Epoch [22/25] Train Loss: 0.4548 | Train Acc: 80.5726 \n",
      "Epoch [23/25] Train Loss: 0.4479 | Train Acc: 82.6176 \n",
      "Epoch [24/25] Train Loss: 0.4335 | Train Acc: 84.6626 \n",
      "Epoch [25/25] Train Loss: 0.4260 | Train Acc: 84.2536 \n"
     ]
    }
   ],
   "source": [
    "dataset = ImageFolder(\"../dataset\", transform=transform)\n",
    "# Note: later found bug here ImageFolder assigns labels in alphabetical order (0=happy, 1=neutral, 2=sad)\n",
    "# This differs from the original training setup (0=happy, 1=sad, 2=neutral)\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model = MyCNN().to(device)\n",
    "state = torch.load(\"models/best_model.pth\", map_location=device, weights_only=True)\n",
    "model.load_state_dict(state, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# freeze early convolutional layers\n",
    "for param in model.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze the last conv layer for fine-tuning\n",
    "for param in model.conv3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# classifier\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# only trainable parameters\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5, weight_decay=0.01\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    train_loss = total_loss / len(train_loader)\n",
    "    train_acc = correct / total * 100\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5feff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated fine-tuned model\n",
    "torch.save(model.state_dict(), \"models/fine_tuned_classifier12.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebcb52",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Iterative Refinement</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ec9c3",
   "metadata": {},
   "source": [
    "Now **systematically** improve performance by adjusting:\n",
    "- Learning rate (coarser -> finer updates)\n",
    "- Number of epochs (underfitting -> appropriate fit -> overfitting)\n",
    "- Unfreezing strategy (conservative -> progressive)\n",
    "\n",
    "Save checkpoints from each round for comparison, and run main.py for real human (me) evaluation, with logging as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a1abcc",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">üêõCritical Bug Fixed: Label Mapping Mismatch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2842b1",
   "metadata": {},
   "source": [
    "To keep it real, during this I found (pretty early on, after couple interactions) a critical bug:\n",
    "\n",
    "**Problem Discovered:**\n",
    "- Original training: Label mapping was `[happy=0, sad=1, neutral=2]`\n",
    "- Fine-tuning with `ImageFolder`: Labels are assigned **alphabetically** by folder names, resulting in `[happy=0, neutral=1, sad=2]`, despite me specifically writing `dataset.class_to_idx = {\"happy\": 0, \"sad\": 1, \"neutral\": 2}`\n",
    "\n",
    "**Impact:**\n",
    "- This mismatch completely confused the model (and me) during fine-tuning\n",
    "- Neutral and sad predictions were being swapped\n",
    "- Explained the unexpectedly poor \"sad\" detection performance\n",
    "\n",
    "**Solution:**\n",
    "- Rename dataset folders to `0_happy`, `1_sad`, `2_neutral` (use numerical prefixes for proper ordering)\n",
    "- Or manually remap labels with a custom Dataset class (didn't want to implement, easier to rename folders)\n",
    "\n",
    "**Lesson:** Always validate label mappings between training and inference to avoid subtle bugs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
