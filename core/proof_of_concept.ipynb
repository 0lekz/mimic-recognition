{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9090e243",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Webcam Proof of Concept</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606df52e",
   "metadata": {},
   "source": [
    "Real-time emotion detection using a pre-trained CNN on live webcam feed. Detects three emotions: Happy, Sad, and Neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "#import cv2, torch, time \n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch\n",
    "import csv\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from playsound3 import playsound\n",
    "from model import MyCNN\n",
    "\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dcd42",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Setup: Initialize Model and Device</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941cba9",
   "metadata": {},
   "source": [
    "Load the pre-trained MyCNN model on GPU (MPS) if available (im on mac), otherwise use CPU. Initialize the Haar Cascade classifier for face detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCNN().to(device)\n",
    "# Load with strict=False to handle any architectural differences\n",
    "model.load_state_dict(torch.load('models/best_model.pth', map_location=device), strict=False)\n",
    "model.eval()\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Emotion labels\n",
    "class_names = ['happy', 'sad', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23ddf1",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\">Face Detection</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16599f",
   "metadata": {},
   "source": [
    "\n",
    "Use Haar Cascade to detect faces in each frame. Defined emotion class labels for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91efa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# labels\n",
    "class_names = ['happy', 'sad', 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5194e",
   "metadata": {},
   "source": [
    "## <p style=\"font-family: Georgia; font-weight: normal; letter-spacing: 2px; color: #fabd2f;     font-size: 140%; text-align: left; padding: 0px; border-bottom: 3px solid #fabd2f;\"> Real-time Emotion Detection Loop</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb717db1",
   "metadata": {},
   "source": [
    "Capture frames from webcam, detect faces, classify emotions, log predictions, and display results. Press 'q' to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d7aeb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy detected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 17:44:54.768 Python[54342:153705] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-11-11 17:44:54.768 Python[54342:153705] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Happy detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n",
      "Sad detected!\n"
     ]
    }
   ],
   "source": [
    "log_path = Path('predictions_log.csv')\n",
    "if not log_path.exists():\n",
    "    with open(log_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"emotion\", \"confidence\"])\n",
    "        \n",
    "cap = cv2.VideoCapture(0) # 0 for default camera\n",
    "pause_until = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    # pause\n",
    "    current_time = time.time()\n",
    "    if current_time < pause_until:\n",
    "        cv2.imshow(\"FER Live\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        continue\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face_resized = cv2.resize(face, (48, 48))\n",
    "        face_tensor = torch.tensor(face_resized).unsqueeze(0).unsqueeze(0).float() / 255.0\n",
    "        face_tensor = (face_tensor - 0.5) / 0.5\n",
    "        face_tensor = face_tensor.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(face_tensor)\n",
    "            probs = F.softmax(output, dim=1)\n",
    "            conf, pred = torch.max(probs, 1)\n",
    "            emotion = class_names[pred.item()]\n",
    "            confidence = conf.item()\n",
    "            \n",
    "        # Log\n",
    "        with open(log_path, \"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([datetime.now().isoformat(), emotion, round(confidence, 4)])\n",
    "            \n",
    "        # Draw\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        text = f\"{emotion} ({confidence*100:.1f}%)\"\n",
    "        cv2.putText(frame, text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,255), 2)\n",
    "        \n",
    "        if emotion == \"sad\":\n",
    "            # CHANGE LATER\n",
    "            #playsound(\"alert.mp3\", block=False)\n",
    "            print(\"Sad detected!\")\n",
    "            pause_until = time.time() + 1.5\n",
    "        elif emotion == \"happy\":\n",
    "            print(\"Happy detected!\")\n",
    "            #playsound(\"happy.mp3\", block=False)\n",
    "            pause_until = time.time() + 1.5\n",
    "            \n",
    "    cv2.imshow(\"FER Live\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
